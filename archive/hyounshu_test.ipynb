{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "file_path = 'keypoints/01/NIA_SL_WORD0001_REAL01_F/NIA_SL_WORD0001_REAL01_F_000000000000_keypoints.json'\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    json_data = json.load(file)\n",
    "# pos:70 / face:70 / \n",
    "if 'pose_keypoints_3d' in json_data['people']:\n",
    "    print(len(json_data['people']['face_keypoints_3d'])//4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "def get_word_list(num_folders_start=8, num_folders_end=9):\n",
    "    folder_path = 'morpheme/01'\n",
    "\n",
    "    # 단어들을 저장할 리스트\n",
    "    word_list = []\n",
    "    \n",
    "    # 파일 이름 얻어오기\n",
    "    file_names = [f for f in os.listdir(folder_path) if f.endswith('.json') and \"F_morpheme\" in f]\n",
    "\n",
    "    # 파일 이름을 번호 순서대로 정렬하기\n",
    "    file_names.sort(key=lambda x: int(x.split('_')[2][4:]))\n",
    "\n",
    "    for idx in range(num_folders_start, num_folders_end + 1):\n",
    "        for filename in file_names:\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "                \n",
    "                # 'data' 키 안의 요소들 순회\n",
    "                for item in data['data']:\n",
    "                    for attribute in item['attributes']:\n",
    "                        word_list.append(attribute['name'])\n",
    "                    \n",
    "\n",
    "    # Label Encoder 초기화 및 학습\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_labels = label_encoder.fit_transform(word_list)\n",
    "    \n",
    "    return encoded_labels\n",
    "\n",
    "print(len(get_word_list()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def get_sequence_files(num_folders_start=1, num_folders_end=7):\n",
    "    base_folder_path = 'keypoints'\n",
    "\n",
    "    # 전체 시퀀스를 저장할 리스트\n",
    "    sequence_files = []\n",
    "\n",
    "    for idx in range(num_folders_start, num_folders_end + 1):\n",
    "\n",
    "        folder_path = os.path.join(base_folder_path, f'{idx:02d}')\n",
    "        \n",
    "        # 각 폴더의 파일 이름을 저장할 리스트\n",
    "        # folder_files = []\n",
    "        \n",
    "        # 파일 이름 얻어오기\n",
    "        file_names = [f for f in os.listdir(folder_path) if \"F\" in f]\n",
    "        \n",
    "        # 파일 이름을 번호 순서대로 정렬하기\n",
    "        file_names.sort(key=lambda x: int(x.split('_')[2][4:]))\n",
    "        \n",
    "        for filename in file_names:\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            json_names = [f for f in os.listdir(file_path) if \"F\" in f]       \n",
    "            json_names.sort(key=lambda x: int(x.split('_')[5]))\n",
    "            \n",
    "            for i, jsonname in enumerate(json_names):\n",
    "                json_path = os.path.join(file_path, jsonname)\n",
    "                json_names[i] = json_path\n",
    "            \n",
    "            sequence_files.append(json_names)\n",
    "        \n",
    "    return sequence_files\n",
    "\n",
    "print(get_sequence_files(1,7)[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keypoints(json_data):\n",
    "    keypoint_types_2d = ['face_keypoints_2d', 'pose_keypoints_2d', 'hand_left_keypoints_2d', 'hand_right_keypoints_2d']\n",
    "    keypoint_types_3d = ['face_keypoints_3d', 'hand_left_keypoints_3d', 'hand_right_keypoints_3d']\n",
    "\n",
    "    # 2D와 3D 키포인트의 총 개수를 계산\n",
    "    num_keypoints_2d = sum(len(json_data['people'][key]) // 3 for key in keypoint_types_2d if key in json_data['people'])\n",
    "    num_keypoints_3d = sum(len(json_data['people'][key]) // 4 for key in keypoint_types_3d if key in json_data['people'])\n",
    "\n",
    "    # Numpy 배열 초기화\n",
    "    keypoints_2d = np.zeros((num_keypoints_2d, 3))  # (x, y, 0)\n",
    "    keypoints_3d = np.zeros((num_keypoints_3d, 3))  # (x, y, z)\n",
    "\n",
    "    def append_coordinates(keypoints_list, array, dimensions, offset=0):\n",
    "        step = dimensions + 1  # dimensions + 1 because of the confidence score\n",
    "        for i in range(0, len(keypoints_list), step):\n",
    "            idx = i // step + offset\n",
    "            if dimensions == 2:\n",
    "                array[idx] = [keypoints_list[i], keypoints_list[i + 1], 0]\n",
    "            elif dimensions == 3:\n",
    "                array[idx] = [keypoints_list[i], keypoints_list[i + 1], keypoints_list[i + 2]]\n",
    "\n",
    "    offset_2d = 0\n",
    "    offset_3d = 0\n",
    "\n",
    "    for key in keypoint_types_2d:\n",
    "        if key in json_data['people']:\n",
    "            append_coordinates(json_data['people'][key], keypoints_2d, dimensions=2, offset=offset_2d)\n",
    "            offset_2d += len(json_data['people'][key]) // 3\n",
    "\n",
    "    for key in keypoint_types_3d:\n",
    "        if key in json_data['people']:\n",
    "            append_coordinates(json_data['people'][key], keypoints_3d, dimensions=3, offset=offset_3d)\n",
    "            offset_3d += len(json_data['people'][key]) // 4\n",
    "\n",
    "    # 필요에 따라 keypoints_2d와 keypoints_3d를 하나의 배열로 합칠 수 있음\n",
    "    keypoints = np.vstack((keypoints_2d, keypoints_3d))\n",
    "    \n",
    "    return keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignLanguageDataset(Dataset):\n",
    "    def __init__(self, sequence_files, labels):\n",
    "        self.data = []\n",
    "        self.labels = labels\n",
    "        for files in sequence_files:\n",
    "            sequence = []\n",
    "            for file in files:\n",
    "                with open(file, 'r') as f:\n",
    "                    json_data = json.load(f)\n",
    "                    keypoints = extract_keypoints(json_data)\n",
    "                    sequence.append(keypoints)\n",
    "            self.data.append(sequence)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        sequence = torch.tensor(self.data[idx], dtype=torch.float32)        \n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        sequence = sequence.permute(2, 0, 1)\n",
    "        return sequence, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_list\n",
    "\n",
    "# 폴더 경로\n",
    "folder_path = 'morpheme/01'\n",
    "\n",
    "# 단어들을 저장할 리스트\n",
    "word_list = []\n",
    "\n",
    "# 파일 이름 얻어오기\n",
    "file_names = [f for f in os.listdir(folder_path) if f.endswith('.json') and \"F_morpheme\" in f]\n",
    "\n",
    "# 파일 이름을 번호 순서대로 정렬하기\n",
    "file_names.sort(key=lambda x: int(x.split('_')[2][4:]))\n",
    "\n",
    "for filename in file_names:\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "        # 'data' 키 안의 요소들 순회\n",
    "        for item in data['data']:\n",
    "            for attribute in item['attributes']:\n",
    "                word_list.append(attribute['name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 임베딩\n",
    "# 예시 한국어 단어 라벨\n",
    "labels = word_list\n",
    "\n",
    "# Label Encoder 초기화 및 학습\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(labels)\n",
    "\n",
    "print(\"Original Labels:\", labels)\n",
    "print(\"Encoded Labels:\", encoded_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence_files \n",
    "import os\n",
    "\n",
    "# 폴더 경로\n",
    "folder_path = 'keypoints/01'\n",
    "\n",
    "# json 파일 경로 저장할 리스트\n",
    "sequence_files = []\n",
    "\n",
    "# 파일 이름 얻어오기\n",
    "file_names = [f for f in os.listdir(folder_path) if \"F\" in f]\n",
    "\n",
    "# 파일 이름을 번호 순서대로 정렬하기\n",
    "file_names.sort(key=lambda x: int(x.split('_')[2][4:]))\n",
    "\n",
    "for filename in file_names:\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    \n",
    "    json_names = [f for f in os.listdir(file_path) if \"F\" in f]       \n",
    "    json_names.sort(key=lambda x: int(x.split('_')[5]))\n",
    "    \n",
    "    for i, jsonname in enumerate(json_names):\n",
    "        json_path = os.path.join(file_path, jsonname)\n",
    "        json_names[i] = json_path\n",
    "    \n",
    "    sequence_files.append(json_names)\n",
    "print(sequence_files)\n",
    "print(file_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "# Collate 함수 정의\n",
    "def collate_fn(batch):\n",
    "    # batch는 keypoints와 labels의 튜플로 구성된 리스트\n",
    "    keypoints, labels = zip(*batch)\n",
    "    \n",
    "    # keypoints는 3D 텐서이므로, 텐서 리스트에서 시퀀스 길이(120)를 추출하여 패딩 처리\n",
    "    keypoints_padded = pad_sequence([k.permute(1, 0, 2) for k in keypoints], batch_first=True, padding_value=0)\n",
    "    \n",
    "    # 패딩 후 다시 원래 차원으로 복원\n",
    "    keypoints_padded = keypoints_padded.permute(0, 2, 1, 3)\n",
    "    \n",
    "    # 각 시퀀스의 길이를 계산 (여기서는 모두 120이 동일함)\n",
    "    lengths = torch.tensor([k.size(1) for k in keypoints])\n",
    "    \n",
    "    # labels를 tensor로 변환\n",
    "    labels = torch.tensor(labels)\n",
    "    \n",
    "    return keypoints_padded, labels, lengths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and DataLoader\n",
    "dataset = SignLanguageDataset(sequence_files, encoded_labels)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (inputs, labels, lengths) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx}:\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Labels: {labels}\")\n",
    "    print(f\"Lengths: {lengths}\")\n",
    "    print(f\"Inputs shape: {inputs.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    \n",
    "    # 배치 2개만 출력하고 종료\n",
    "    if batch_idx == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, dim_model, dropout_p, max_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "        \n",
    "        # 최대 길이에 대한 Positional Encoding 생성\n",
    "        pos_encoding = torch.zeros(max_len, dim_model)\n",
    "        positions_list = torch.arange(0, max_len, dtype=torch.float).view(-1, 1) # 0, 1, 2, 3, ...\n",
    "        division_term = torch.exp(torch.arange(0, dim_model, 2).float() * (-math.log(10000.0)) / dim_model)\n",
    "        \n",
    "        pos_encoding[:, 0::2] = torch.sin(positions_list * division_term)\n",
    "        pos_encoding[:, 1::2] = torch.cos(positions_list * division_term)\n",
    "        \n",
    "        # Positional Encoding을 모델의 버퍼로 등록\n",
    "        pos_encoding = pos_encoding.unsqueeze(0)\n",
    "        self.register_buffer(\"pos_encoding\", pos_encoding)\n",
    "    \n",
    "    def forward(self, token_embedding: torch.tensor) -> torch.tensor:\n",
    "        seq_len = token_embedding.size(1)\n",
    "        pos_encoding = self.pos_encoding[:, :seq_len, :]\n",
    "        return self.dropout(token_embedding + pos_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_layers, num_classes):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_fc = nn.Linear(input_dim, model_dim)\n",
    "        \n",
    "        # Positional Encoding 추가\n",
    "        self.positional_encoding = PositionalEncoding(dim_model=model_dim, dropout_p=0.1, max_len=500)\n",
    "        \n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.fc = nn.Linear(model_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x, src_key_padding_mask):\n",
    "        x = self.input_fc(x)\n",
    "        \n",
    "        # Positional Encoding 적용\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)  # transformer 대신 transformer_encoder 사용\n",
    "        x = x.mean(dim=1)  # 시퀀스 차원 축소\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# 모델 초기화\n",
    "input_dim = 249 * 3 # 각 키포인트의 2D 좌표(2)와 3D 좌표(3)를 사용\n",
    "model_dim = 512  # 모델 차원\n",
    "num_heads = 8  # 멀티헤드 어텐션의 헤드 수\n",
    "num_layers = 3  # Transformer 레이어 수\n",
    "num_classes = 2771  # 출력 클래스 수\n",
    "\n",
    "model = TransformerModel(input_dim, model_dim, num_heads, num_layers, num_classes)\n",
    "\n",
    "learning_rate = 0.0001\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "loss_values = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for sequences, labels, lengths in dataloader:\n",
    "        \n",
    "        mean = torch.mean(sequences)\n",
    "        std = torch.std(sequences)\n",
    "        sequences = (sequences - mean) / std\n",
    "        \n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # 마스킹 생성\n",
    "        src_key_padding_mask = (sequences.sum(dim=-1) == 0)\n",
    "        src_key_padding_mask = src_key_padding_mask.any(dim=1).to(device)\n",
    "\n",
    "        # 입력 텐서 변환: [batch_size, 3, seq_len, num_joints] -> [batch_size, seq_len, 3 * num_joints]\n",
    "        batch_size, coord, seq_len, num_joints = sequences.size()\n",
    "        sequences = sequences.permute(0, 2, 3, 1).contiguous()  # [batch_size, seq_len, num_joints, coord]\n",
    "        sequences = sequences.view(batch_size, seq_len, -1)  # [batch_size, seq_len, num_joints * coord]\n",
    "\n",
    "        # Forward pass\n",
    "        model.to(device)\n",
    "        outputs = model(sequences, src_key_padding_mask=src_key_padding_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # 에폭당 loss 값을 기록합니다.\n",
    "    loss_values.append(loss.item())\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "\n",
    "print('Training finished.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for sequences, labels, lengths in dataloader:\n",
    "        \n",
    "        mean = torch.mean(sequences)\n",
    "        std = torch.std(sequences)\n",
    "        sequences = (sequences - mean) / std\n",
    "        \n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # 마스킹 생성\n",
    "        src_key_padding_mask = (sequences.sum(dim=-1) == 0)\n",
    "        src_key_padding_mask = src_key_padding_mask.any(dim=1).to(device)\n",
    "\n",
    "        # 입력 텐서 변환: [batch_size, 3, seq_len, num_joints] -> [batch_size, seq_len, 3 * num_joints]\n",
    "        batch_size, coord, seq_len, num_joints = sequences.size()\n",
    "        sequences = sequences.permute(0, 2, 3, 1).contiguous()  # [batch_size, seq_len, num_joints, coord]\n",
    "        sequences = sequences.view(batch_size, seq_len, -1)  # [batch_size, seq_len, num_joints * coord]\n",
    "\n",
    "        # Forward pass\n",
    "        model.to(device)\n",
    "        outputs = model(sequences, src_key_padding_mask=src_key_padding_mask)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # 에폭당 loss 값을 기록합니다.\n",
    "    loss_values.append(loss.item())\n",
    "    print(f'Epoch [{epoch+100}/{num_epochs+100}], Loss: {loss.item():.4f}')\n",
    "    \n",
    "\n",
    "print('Training finished.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Loss 값 시각화\n",
    "plt.figure(figsize=(30, 10))\n",
    "plt.plot(range(1, len(loss_values) + 1), loss_values, marker='o', linestyle='-', color='b')\n",
    "plt.title('Training Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 모델을 평가 모드로 전환\n",
    "model.eval()\n",
    "\n",
    "# 평가 데이터를 로드 (학습 데이터와 동일한 데이터로 테스트하는 경우 예제)\n",
    "# dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# 평가 지표 초기화\n",
    "correct = 0\n",
    "total = 0\n",
    "predict = []\n",
    "\n",
    "# 평가 모드에서 그라디언트 계산 비활성화\n",
    "with torch.no_grad():\n",
    "    for inputs, labels, lengths in dataloader:\n",
    "        # 모델 출력 계산\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        batch_size, coord, seq_len, num_joints = inputs.size()\n",
    "        inputs = inputs.permute(0, 2, 3, 1).contiguous()  # [batch_size, seq_len, num_joints, coord]\n",
    "        inputs = inputs.view(batch_size, seq_len, -1)  # [batch_size, seq_len, num_joints * coord]\n",
    "        \n",
    "        # 패딩 마스크 생성\n",
    "        src_key_padding_mask = create_padding_mask(inputs[:,:,0])\n",
    "        src_key_padding_mask = src_key_padding_mask.to(device)\n",
    "        \n",
    "        outputs = model(inputs, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        # 소프트맥스 함수로 확률로 변환\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        \n",
    "        # 가장 높은 확률을 가진 클래스의 인덱스 구하기\n",
    "        _, predicted_classes = torch.max(probabilities, 1)\n",
    "        \n",
    "        # 정확도 계산\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted_classes == labels).sum().item()\n",
    "                \n",
    "        \n",
    "        # 예측된 클래스 인덱스를 원래 라벨로 변환\n",
    "        for predicted_class, label in zip(predicted_classes, labels):\n",
    "            predicted_label = word_list[predicted_class.item()]\n",
    "            actual_label = word_list[label.item()]\n",
    "            predict.append(predicted_label)\n",
    "            # 출력\n",
    "            print(f\"Actual Label (encoded): {label.item()}\")\n",
    "            print(f\"Predicted Label (encoded): {predicted_class.item()}\")\n",
    "            print(f\"Actual Label: {actual_label}\")\n",
    "            print(f\"Predicted Label: {predicted_label}\")\n",
    "            print('-' * 30)\n",
    "\n",
    "# 전체 정확도 출력\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the model on the test data: {accuracy:.2f}%')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(set(predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Loss 값 시각화\n",
    "# plt.figure(figsize=(50, 25))\n",
    "# plt.plot(range(1, len(loss_values) + 1), loss_values, marker='o', linestyle='-', color='b')\n",
    "# plt.title('Training Loss over Epochs')\n",
    "# plt.xlabel('Epoch')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.grid(True)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# print(torch.__version__)\n",
    "# print(torch.cuda.is_available())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
