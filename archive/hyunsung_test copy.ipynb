{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 필요한 모듈 선언 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 필요한 모듈 선언\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "print('device : ', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = 'morpheme/01'\n",
    "\n",
    "# 단어들을 저장할 리스트\n",
    "word_list = []\n",
    "\n",
    "# 파일 이름 얻어오기\n",
    "file_names = [f for f in os.listdir(folder_path) if f.endswith('.json') and \"F_morpheme\" in f]\n",
    "\n",
    "# 파일 이름을 번호 순서대로 정렬하기\n",
    "file_names.sort(key=lambda x: int(x.split('_')[2][4:]))\n",
    "\n",
    "for filename in file_names:\n",
    "    file_path = os.path.join(folder_path, filename)\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "        \n",
    "        # 'data' 키 안의 요소들 순회\n",
    "        for item in data['data']:\n",
    "            for attribute in item['attributes']:\n",
    "                word_list.append(attribute['name'])\n",
    "\n",
    "# 결과 출력 \n",
    "print(len(word_list))\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(word_list)\n",
    "# label_mapping 딕셔너리 생성\n",
    "label_mapping = {f\"NIA_SL_WORD{str(i+1).zfill(4)}_REAL01_F\": encoded_labels[i] for i,word in enumerate(word_list)}\n",
    "print(label_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_encoder.inverse_transform([1941]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 클래스 선언 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 임시!!!!!\n",
    "class SignLanguageDataset(Dataset):\n",
    "    def __init__(self, data_dir, folder_to_label):\n",
    "        self.data_dir = data_dir\n",
    "        self.folder_to_label = folder_to_label\n",
    "        self.data, self.labels = self.load_data()\n",
    "\n",
    "    def load_data(self):\n",
    "        file_list = []\n",
    "        labels = []\n",
    "        for folder_name in os.listdir(self.data_dir):\n",
    "            if folder_name.endswith(\"F\") and folder_name in self.folder_to_label:  # \"F\"로 끝나는 폴더만 처리\n",
    "                label = self.folder_to_label[folder_name]\n",
    "                label = int(label)\n",
    "                label_name = label_encoder.inverse_transform([label])\n",
    "                folder_path = os.path.join(self.data_dir, folder_name)\n",
    "                if os.path.isdir(folder_path):\n",
    "                    json_files = [os.path.join(folder_path, file_name) for file_name in os.listdir(folder_path) if file_name.endswith('.json')]\n",
    "                    file_list.append(json_files)\n",
    "                    labels.append(label)\n",
    "                    print(f\"Label: {label_name},label_num: {label} Folder: {folder_name}, Frame count: {len(json_files)}\")\n",
    "        return file_list, labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        all_keypoints = []\n",
    "        \n",
    "        json_file_list = self.data[index]\n",
    "        label = self.labels[index]\n",
    "        for file_path in json_file_list:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            frame_data = data['people']\n",
    "            if frame_data:\n",
    "                keypoints_2d = np.array(frame_data['face_keypoints_2d'] +\n",
    "                                        frame_data['pose_keypoints_2d'] +\n",
    "                                        frame_data['hand_left_keypoints_2d'] +\n",
    "                                        frame_data['hand_right_keypoints_2d'])\n",
    "                keypoints_2d = keypoints_2d.reshape(-1, 3)[:, :2].flatten()  # (num_keypoints * 2,)\n",
    "                \n",
    "                keypoints_3d = np.array(frame_data['face_keypoints_3d'] +\n",
    "                                        frame_data['pose_keypoints_3d'] +\n",
    "                                        frame_data['hand_left_keypoints_3d'] +\n",
    "                                        frame_data['hand_right_keypoints_3d'])\n",
    "                keypoints_3d = keypoints_3d.reshape(-1, 4)[:, :3].flatten() \n",
    "                \n",
    "                keypoints = np.concatenate((keypoints_2d, keypoints_3d))\n",
    "                all_keypoints.append(keypoints)\n",
    "        \n",
    "        all_keypoints = np.array(all_keypoints)  # (num_frames, num_keypoints * (2 + 3))\n",
    "        return torch.tensor(all_keypoints, dtype=torch.float32), torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = \"keypoints/01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(os.listdir(root_dir))\n",
    "label = int((''.join(filter(str.isdigit, 'NIA_SL_WORD0101_REAL01_L')))[0:4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(label_mapping)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SignLanguageDataset(root_dir, label_mapping)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch_idx, (inputs, labels) in enumerate(dataloader):\n",
    "    print(f\"Batch {batch_idx}:\")\n",
    "    print(f\"Inputs: {inputs}\")\n",
    "    print(f\"Labels: {labels}\")\n",
    "    print(f\"Inputs shape: {inputs.shape}\")\n",
    "    print(f\"Labels shape: {labels.shape}\")\n",
    "    \n",
    "    # 배치 2개만 출력하고 종료\n",
    "    if batch_idx == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 선언 ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 하이퍼파라미터 설정\n",
    "num_keypoints = 138  # face, pose, hand_left, hand_right keypoints\n",
    "input_dim = 685 # 각 키포인트의 2D 좌표(2)와 3D 좌표(3)를 사용\n",
    "model_dim = 512  # 모델 차원\n",
    "num_heads = 16  # 멀티헤드 어텐션의 헤드 수\n",
    "num_layers = 8  # Transformer 레이어 수\n",
    "num_classes = 2771  # 출력 클래스 수\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, model_dim, num_heads, num_layers, num_classes):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.input_fc = nn.Linear(input_dim, model_dim)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model=model_dim, nhead=num_heads,batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        self.fc = nn.Linear(model_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.input_fc(x)\n",
    "        x = self.transformer_encoder(x)  # transformer 대신 transformer_encoder 사용\n",
    "        x = x.mean(dim=1)  # 시퀀스 차원 축소\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "\n",
    "model = TransformerModel(input_dim, model_dim, num_heads, num_layers, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 손실 함수 및 옵티마이저 설정\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "model.train()\n",
    "min_loss = 100\n",
    "num_epochs = 100\n",
    "num_batches_to_train = 100  # 학습시킬 데이터 수\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    batch_count = 0\n",
    "    for inputs, labels in dataloader:\n",
    "        \n",
    "        mean = torch.mean(inputs)\n",
    "        std = torch.std(inputs)\n",
    "        inputs = (inputs - mean) / std\n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # 입력 데이터와 라벨을 GPU로 전송\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        ll = loss.item()\n",
    "        if ll < min_loss:\n",
    "            min_loss = ll\n",
    "        \n",
    "        batch_count += 1\n",
    "        if batch_count >= num_batches_to_train:\n",
    "            break\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print(f'Minimum Loss: {min_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "# 모델을 평가 모드로 전환\n",
    "model.eval()\n",
    "\n",
    "# 평가 데이터를 로드 (학습 데이터와 동일한 데이터로 테스트하는 경우 예제)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# 평가 지표 초기화\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# 평가 모드에서 그라디언트 계산 비활성화\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in dataloader:\n",
    "        \n",
    "        mean = torch.mean(inputs)\n",
    "        std = torch.std(inputs)\n",
    "        inputs = (inputs - mean) / std\n",
    "        \n",
    "        inputs, labels = inputs.to(device), labels.to(device)  # 입력 데이터와 라벨을 GPU로 전송\n",
    "        \n",
    "        # 모델 출력 계산\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # 소프트맥스 함수로 확률로 변환\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)\n",
    "        \n",
    "        # 가장 높은 확률을 가진 클래스의 인덱스 구하기\n",
    "        _, predicted_class = torch.max(probabilities, 1)\n",
    "        \n",
    "        # 정확도 계산\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted_class == labels).sum().item()\n",
    "        \n",
    "        # 예측된 클래스 인덱스를 원래 라벨로 변환\n",
    "        predicted_label = word_list[predicted_class.item()]\n",
    "        \n",
    "        # 출력\n",
    "        print(f\"Actual Label (encoded): {labels.item()}\")\n",
    "        print(f\"Predicted Label (encoded): {predicted_class.item()}\")\n",
    "        print(f\"Predicted Label: {predicted_label}\")\n",
    "        print('-' * 30)\n",
    "        \n",
    "\n",
    "# 전체 정확도 출력\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Accuracy of the model on the test data: {accuracy:.2f}%')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
